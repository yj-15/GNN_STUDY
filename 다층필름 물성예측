import pandas as pd
import numpy as np
import os
from functools import partial
from itertools import combinations
from collections import defaultdict
from tqdm import tqdm

import optuna
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, mean_absolute_error
from sklearn.model_selection import GridSearchCV, train_test_split, LeaveOneOut, KFold
from sklearn.multioutput import MultiOutputRegressor
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import ElasticNet
from sklearn.cross_decomposition import PLSRegression

from autofeat import AutoFeatRegressor, FeatureSelector
from autofeat.featsel import select_features
from autofeat.feateng import engineer_features

import matplotlib.pyplot as plt
import json

import warnings
warnings.filterwarnings('ignore')

## Data Preprocessing: main_feature / backup_feature 분리

data_raw = pd.read_csv(r'D:\#.Secure Work Folder\다층필름 물성 예측\220930_multilayer_properties.csv', index_col=0).loc[:81]
features_all = pd.read_csv(r'D:\#.Secure Work Folder\다층필름 물성 예측\220914_singlayer_properties.csv', index_col=0)
multilayer_data_path = None
engineered_data_path = None

main_features = ['MD_yield_stress','MD_failure','MD_1%_modulus','TD_yield_stress','TD_failure','TD_1%_modulus']
backup_features = [i for i in features_all.columns if i not in main_features]


## PLS 모듈 import

def fit_pls(data_split, **kwargs):
    model = PLSRegression(**kwargs)
    model.fit(data_split['X_train_scaled'], data_split['y_train_scaled'])
    score = model.score(data_split['X_test_scaled'], data_split['y_test_scaled'])

    return model, score

def feature_engineering(data_split, max_step=3):
    added_columns = pd.DataFrame()
    for steps in range(max_step):
        print(f"AutoFeat with {steps} feateng_steps")
        model = AutoFeatRegressor(verbose=1, feateng_steps=steps)
        d = model.fit_transform(data_split['X_train'], data_split['y_train'])
        r2 = model.score(data_split['X_test'], data_split['y_test'])
        print(f"Final R^2: {r2}")

        new_data = model.transform(data_split['raw_data'])
        added_columns = pd.concat(
            [added_columns, new_data[[c for c in new_data.columns if c not in added_columns.columns.to_list() + data_split['raw_data'].columns.to_list()]]], axis=1)
    return added_columns

def split_data(X, y, test_size, **kwargs):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, **kwargs)

    scaler_x = StandardScaler()
    scaler_y = StandardScaler()

    X_train_scaled = pd.DataFrame(scaler_x.fit_transform(X_train), columns=X_train.columns)
    X_test_scaled = pd.DataFrame(scaler_x.transform(X_test), columns=X_test.columns)
    y_train_scaled = pd.DataFrame(scaler_y.fit_transform(y_train.values.reshape(-1, 1)), columns=['drop_impact'])
    y_test_scaled = pd.DataFrame(scaler_y.transform(y_test.values.reshape(-1, 1)), columns=['drop_impact'])

    raw_data = pd.DataFrame(scaler_x.transform(X), columns=X.columns)
    raw_target = pd.DataFrame(scaler_y.transform(y.values.reshape(-1, 1)), columns=['drop_impact'])

    data_split = {'raw_data': raw_data,
                  'raw_target': raw_target,
                  'X_train': X_train,
                  'X_test': X_test,
                  'y_train': y_train,
                  'y_test': y_test,
                  'X_train_scaled': X_train_scaled,
                  'X_test_scaled': X_test_scaled,
                  'y_train_scaled': y_train_scaled,
                  'y_test_scaled': y_test_scaled,
                  'scaler_x': scaler_x,
                  'scaler_y': scaler_y
                  }

    return data_split

def fit_5fold_model(fit_func, params, data_raw, target_raw):
    # Leave-One-Out score
    kfold = KFold(n_splits=5, shuffle=True)
    y_pred, y_real = [], []

    for train_idx, test_idx in kfold.split(X=data_raw, y=target_raw):
        data_temp = {'X_train_scaled': data_raw.iloc[train_idx],
                     'X_test_scaled': data_raw.iloc[test_idx],
                     'y_train_scaled': target_raw.iloc[train_idx],
                     'y_test_scaled': target_raw.iloc[test_idx]}

        model, score = fit_func(data_temp, **params)

        y_pred.append(model.predict(data_temp['X_test_scaled']))
        y_real.append(data_temp['y_test_scaled'].values)

    y_pred = np.concatenate(y_pred)
    y_real = np.concatenate(y_real)

    return y_real, y_pred

model_type = "pls"
if model_type == "pls":
    fit_func = fit_pls


## PLS 모델 성능을 기반으로 best combination feature 추출

PLS_COMBI_RESULT = pd.DataFrame(columns = ['r2_score','mae_score'])
r2_pls, mae_pls, index, y_real, y_pred = [],[],[],[],[]
for i in tqdm(range(0, len(backup_features)+1)):
    for combi in list(combinations(backup_features, i)):
        feature_list = main_features + list(combi)
        index.append(''.join(list(combi)))
        features = features_all[[i for i in feature_list]]
        
        # input data 생성 과정
        # multilayer 각 층 정보를 single layer property로부터 얻어옴
        # 3층이 없을 경우 존재하므로 2와 3의 최소 공배수인 6으로 맞추어주고자 반복 수행
        if multilayer_data_path is None:
            multilayer_features = pd.DataFrame()
            for idx, row in data_raw.iterrows(): # multilayer properties
                if pd.isna(row["3rd_layer"]):
                    for i in [0, 1, 2]: # iteration 이유
                        for featname, value in features.loc[row['1st_layer']].iteritems(): #single layer properties
                            multilayer_features.loc[idx, f'{i}th_{featname}'] = value
                        multilayer_features.loc[idx, f'{i}th_thickness'] = float(row['1st_layer'].split('_')[-1].split('um')[0])/3
                    for i in [3, 4, 5]:
                        for featname, value in features.loc[row['2nd_layer']].iteritems():
                            multilayer_features.loc[idx, f'{i}th_{featname}'] = value
                        multilayer_features.loc[idx, f'{i}th_thickness'] = float(row['2nd_layer'].split('_')[-1].split('um')[0]) / 3
                else:
                    for i in [0, 1]:
                        for featname, value in features.loc[row['1st_layer']].iteritems():
                            multilayer_features.loc[idx, f'{i}th_{featname}'] = value
                        multilayer_features.loc[idx, f'{i}th_thickness'] = float(row['1st_layer'].split('_')[-1].split('um')[0]) / 2
                    for i in [2, 3]:
                        for featname, value in features.loc[row['2nd_layer']].iteritems():
                            multilayer_features.loc[idx, f'{i}th_{featname}'] = value
                        multilayer_features.loc[idx, f'{i}th_thickness'] = float(row['2nd_layer'].split('_')[-1].split('um')[0]) / 2
                    for i in [4, 5]:
                        for featname, value in features.loc[row['3rd_layer']].iteritems():
                            multilayer_features.loc[idx, f'{i}th_{featname}'] = value
                        multilayer_features.loc[idx, f'{i}th_thickness'] = float(row['3rd_layer'].split('_')[-1].split('um')[0]) / 2

                multilayer_features.loc[idx, 'drop_impact'] = row['drop impact']
                multilayer_features.loc[idx, 'DI_recal'] = row['DI_recal']
                multilayer_features.loc[idx, 'DI_recal_median'] = row['DI_recal_median']
            
        # 추출한 data
        data_all = multilayer_features[[c for c in multilayer_features.columns if c not in  ['drop_impact', 'DI_recal', 'DI_recal_median']]]
        target = multilayer_features['DI_recal_median']
        data_all_split = split_data(data_all, target, test_size=0.2, shuffle=True, random_state=34)
        
        # grid search
        pls = PLSRegression()
        parameters = {'n_components': [3,4,5,6]}
        pls_grid = GridSearchCV(pls,parameters,n_jobs = 5,verbose=True)
        pls_grid.fit(data_all_split['X_train_scaled'],data_all_split['y_train_scaled'])
        
        
        # 성능 추출
        y_real_pls, y_pred_pls = fit_5fold_model(fit_pls, pls_grid.best_params_, data_all, target)
        y_real.append(y_real_pls)
        y_pred.append(y_pred_pls)
        r2_pls.append(r2_score(y_real_pls, y_pred_pls))
        mae_pls.append(mean_absolute_error(y_real_pls, y_pred_pls))
        
  PLS_COMBI_RESULT = PLS_COMBI_RESULT.reindex(index = index)

final_pred = []
for i in range(len(y_pred)):
    final_pred.append(np.array(y_pred[i]).reshape(1,-1)[0])
final_real = []
for i in range(len(y_real)):
    final_real.append(np.array(y_real[i]).reshape(1,-1)[0])

PLS_COMBI_RESULT['r2_score'] = r2_pls
PLS_COMBI_RESULT['mae_score'] = mae_pls
PLS_COMBI_RESULT['pred'] = final_pred
PLS_COMBI_RESULT['real'] = final_real
PLS_COMBI_RESULT.to_csv('221123_pls_result.csv')

PLS_COMBI_RESULT = pd.read_csv('221123_pls_result.csv')
ind = []
for i in tqdm(range(0, len(backup_features)+1)):
    for combi in list(combinations(backup_features, i)):
        tmp = ''
        if len(list(combi)) > 1:
            for j in list(combi):
                tmp += j
                tmp += '/'
            ind.append(tmp[:-1])
        else:
            for j in list(combi):
                tmp += j
            ind.append(tmp)
PLS_COMBI_RESULT = PLS_COMBI_RESULT.drop(PLS_COMBI_RESULT.columns[0], axis = 1)
PLS_COMBI_RESULT = PLS_COMBI_RESULT.set_index(np.array(ind))

th_r2, th_mae, _,_ = PLS_COMBI_RESULT.iloc[-1].values
# 기준이 r2_score보다 높을 때의 변수 조합 list 추출
feature_tmp = []
for i,j in enumerate(PLS_COMBI_RESULT['r2_score'].values):
    if j > th_r2:
        feature_tmp.append(PLS_COMBI_RESULT['r2_score'].index[i])
# 기준이 되는 mae 보다 낮은 변수 조합 list 추출
feature_selec = []
for id in feature_tmp:
    if PLS_COMBI_RESULT.loc[id][1] < th_mae:
        feature_selec.append(id)
 
 # 변수 조합 3개 이하일 때 최고 성능 추출
ind, x, y1, y2, pred, real = [],[],[],[],[],[]
for indexes in feature_selec:
    if len([ind for ind in indexes.split('/')]) <= 3:
        ind.append(indexes)
        x.append([ind for ind in indexes.split('/')])
        y1.append(PLS_FEATURE_SELEC.loc[indexes][0])
        y2.append(PLS_FEATURE_SELEC.loc[indexes][1])

plt.figure(figsize=(20, 5))
plt.subplot(121)
plt.plot(y1,'o-')
plt.title('r2_score')

plt.subplot(122)
plt.plot(y2,'o-')
plt.title('mae_score') 

print('변수 조합이 3개 이하인 index 중 r2 score가 높은 변수 조합은 {}입니다'.format(x[y1.index(max(y1))]))
print('해당하는 변수 조합에서의 r2_score와 mae_score는 각각 {}, {}입니다'.format(y1[y1.index(max(y1))], y2[y1.index(max(y1))]))
print('변수 조합이 3개 이하인 index 중 mae score가 낮은 변수 조합은 {}입니다'.format(x[y2.index(min(y2))]))
print('해당하는 변수 조합에서의 r2_score와 mae_score는 각각 {}, {}입니다'.format(y1[y2.index(min(y2))], y2[y2.index(min(y2))]))
 

## feature 최종 채택 후 pls 모델 define

feature_extraction = ['MD_yield_point', 'MD_strain_hardening_modulus', 'MD_toughness']
main_features  += feature_extraction
final_features = features[[i for i in main_features]]
if multilayer_data_path is None:
    multilayer_features = pd.DataFrame()
    for idx, row in data_raw.iterrows(): # multilayer properties
        if pd.isna(row["3rd_layer"]):
                    for i in [0, 1, 2]: # iteration 이유
                        for featname, value in final_features.loc[row['1st_layer']].iteritems(): #single layer properties
                            multilayer_features.loc[idx, f'{i}th_{featname}'] = value
                        multilayer_features.loc[idx, f'{i}th_thickness'] = float(row['1st_layer'].split('_')[-1].split('um')[0])/3
                    for i in [3, 4, 5]:
                        for featname, value in final_features.loc[row['2nd_layer']].iteritems():
                            multilayer_features.loc[idx, f'{i}th_{featname}'] = value
                        multilayer_features.loc[idx, f'{i}th_thickness'] = float(row['2nd_layer'].split('_')[-1].split('um')[0]) / 3
        else:
                    for i in [0, 1]:
                        for featname, value in final_features.loc[row['1st_layer']].iteritems():
                            multilayer_features.loc[idx, f'{i}th_{featname}'] = value
                        multilayer_features.loc[idx, f'{i}th_thickness'] = float(row['1st_layer'].split('_')[-1].split('um')[0]) / 2
                    for i in [2, 3]:
                        for featname, value in final_features.loc[row['2nd_layer']].iteritems():
                            multilayer_features.loc[idx, f'{i}th_{featname}'] = value
                        multilayer_features.loc[idx, f'{i}th_thickness'] = float(row['2nd_layer'].split('_')[-1].split('um')[0]) / 2
                    for i in [4, 5]:
                        for featname, value in final_features.loc[row['3rd_layer']].iteritems():
                            multilayer_features.loc[idx, f'{i}th_{featname}'] = value
                        multilayer_features.loc[idx, f'{i}th_thickness'] = float(row['3rd_layer'].split('_')[-1].split('um')[0]) / 2

        multilayer_features.loc[idx, 'drop_impact'] = row['drop impact']
        multilayer_features.loc[idx, 'DI_recal'] = row['DI_recal']
        multilayer_features.loc[idx, 'DI_recal_median'] = row['DI_recal_median']
  
 # 추출한 data
data_all = multilayer_features[[c for c in multilayer_features.columns if c not in  ['drop_impact', 'DI_recal', 'DI_recal_median']]]
target = multilayer_features['DI_recal_median']
data_all_split = split_data(data_all, target, test_size=0.2, shuffle=True, random_state=34)
        
# grid search
pls = PLSRegression()
parameters = {'n_components': [3,4,5,6]}
pls_grid = GridSearchCV(pls,parameters,n_jobs = 5,verbose=True)
pls_grid.fit(data_all_split['X_train_scaled'],data_all_split['y_train_scaled'])
        
        
# 성능 추출
y_real_pls, y_pred_pls = fit_5fold_model(fit_pls, pls_grid.best_params_, data_all, target)
print(r2_score(y_real_pls, y_pred_pls))
print(mean_absolute_error(y_real_pls, y_pred_pls))

pls, _ = fit_pls(data_all_split, **pls_grid.best_params_)

# 변수 중요도 추출
# y_loadings
comp_importance = pls.y_loadings_
comp_importance = pd.DataFrame(list(zip([i for i in range(1, 7)],comp_importance.flatten())),columns=['n_component','feature_importance'])
comp_importance = comp_importance.set_index(keys = ['n_component'])
comp_importance.sort_values(by=['feature_importance'],ascending=False,inplace=True)
comp_importance.plot.bar()
# coefficent
importance = pls.coef_.squeeze()
feature_importance = pd.DataFrame(list(zip(data_all.columns,importance)),columns=['col_name','feature_importance_vals'])
feature_importance.sort_values(by=['feature_importance_vals'],ascending=False,inplace=True)
final_imp_dict = defaultdict(float)
for i,j in enumerate(feature_importance['col_name']):
    name = ''
    for k in j.split('_')[1:]:
        name += k
        name += '_'
    final_imp_dict[name[:-1]] += feature_importance['feature_importance_vals'].iloc[i]
final_imp_df = pd.DataFrame(index = final_imp_dict.keys(), data = final_imp_dict.values(), columns = ['coefficient'])

plt.figure(figsize=(8,6))
plt.rc('font', family='LG Smart_H')
plt.rc('font', size = 10)
plt.xlabel('변수 중요도')
plt.ylabel('변수명')
plt.barh(final_imp_df.index, final_imp_df['coefficient'].values)
